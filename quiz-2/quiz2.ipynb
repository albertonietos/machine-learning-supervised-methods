{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quiz 2\n",
    "Notebook containing the answers for quiz 2 in ML: Supervised Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1\n",
    "\n",
    "### (a) What statement / which statements are correct about PAC learnability?\n",
    "* (i) The underlying distribution is fixed but unknown $\\rightarrow$ **TRUE**, we assume the samples to be drawn from the same distribution $\\mathcal{D}$ which is not known.\n",
    "* (ii) Generalization error bound gives the the expected generalization error on a fixed distribution generating the data $\\rightarrow$ **FALSE**, the generalization error bound depends on sample size $m$ and level of confidence $\\delta$. It gives the generalization errror for any distribution generating the data.\n",
    "* (iii) The examples should be independently drawn from an identical distribution $\\rightarrow$ **TRUE**, we assume the future examples are independetly drawn from the same underlying distribution $\\mathcal{D}$ (i.i.d assumption).\n",
    "\n",
    "### (b) What statement / which statements are correct about the Bayes error?\n",
    "* (i) For a fixed distribution generating the data, Bayes error cannot be reduced $\\rightarrow$ **TRUE**, for any hypothesis given a distribution \\mathcal{D} the Bayes error cannot be reduced.\n",
    "* (ii) Bayes error gives the expected noise level $\\rightarrow$ **TRUE**, the expectation of the noise is the Bayes error.\n",
    "* (iii) It is possible to construct an optimal learner with a lower error than the Bayes error $\\rightarrow$ **FALSE**, the Bayes error is the minimum achievable error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2\n",
    "\n",
    "## (a) \n",
    "Based on the generalization bound relying on the size of the hypothesis class using boolean conjunctions, and the following information, what is the lower bound on the number of examples?\n",
    "\n",
    "(Formula: $m \\geq \\dfrac{1}{\\epsilon} \\left(\\log(|\\mathcal{H}|) + \\log\\left(\\dfrac{1}{\\delta}\\right)\\right)$ in which the logarithms are natural.)\n",
    "\n",
    "Dataset : $3$ binary features and one binary label\n",
    "\n",
    "Error bound : $8\\%$\n",
    "\n",
    "Confidence level : $96%$ ($\\delta = 4\\%$)\n",
    "\n",
    "**Note**: to have the bound satisfied the fractional values should be rounded up.\n",
    "* (i) $82 \\rightarrow$ **CORRECT**\n",
    "* (ii) $157$\n",
    "* (iii) $63$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "82.0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = 3  # number of binary features\n",
    "eps = 0.08  # error bound (level of generalization eror)\n",
    "delta = 0.04  # level of confidence\n",
    "m = np.ceil(1/eps * (np.log(3**d)  + np.log(1/delta)))\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (b) \n",
    "Based on the generalization bound for true error, using the empirical error\n",
    "$$R(h) \\leq \\hat{R}(h) + \\sqrt{\\dfrac{\\log\\frac{2}{\\delta}}{2m}},$$\n",
    "if we change $\\delta$ from $0.04$ to $0.08$, how many examples will be needed to keep the same bound as before?\n",
    "* (i) $0.5m$\n",
    "* (ii) $1.6m$\n",
    "* (iii) $0.8m$ $\\rightarrow$ **CORRECT**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Knowing that the empirical error should remain the same, we need only to look at the square root term to figure out how does a change in $\\delta$ would change the required $m$ to keep the result of that square root the same.\n",
    "Thus,\n",
    "$$\\sqrt{\\dfrac{\\log\\frac{2}{\\delta_1}}{2m_1}}=\\sqrt{\\dfrac{\\log\\frac{2}{\\delta_2}}{2m_2}}.$$\n",
    "We know $\\delta_1=0.04$ and $\\delta_2=0.08$.\n",
    "\n",
    "Doing some simple math, we get that\n",
    "$$\\dfrac{m_2}{m_1}=\\dfrac{\\log(1/\\delta_1)}{\\log(2/\\delta_1)}=0.823.$$\n",
    "\n",
    "Finally, $m_2 = 0.8 * m_1.$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3\n",
    "\n",
    "## (a)  What statement / which statements are correct about the VC dimension?\n",
    "* (i) VC dimension is dependent of the training dataset\n",
    "* (ii) If VC dimension of a class of functions is $m$, then all possible datasets of size $m$ can be shattered\n",
    "* (iii) VC dimension measures the ability of the classifiers from the hypothesis set to fit all the possible label configurations of at least one set of data samples of size $m$\n",
    "\n",
    "## (b) What statement / which statements are correct about the Rademacher complexity?\n",
    "* (i) Rademacher complexity can be checked empirically for a given dataset\n",
    "* (ii) Rademacher complexity depends on the distribution generating the data\n",
    "* (iii) Rademacher complexity measures the performance of the learning algorithm in the worst-case scenario of assigning labels to samples in adversarial way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
